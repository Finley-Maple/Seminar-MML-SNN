{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-17 09:20:05.628624: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-11-17 09:20:05.944322: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-11-17 09:20:05.944379: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-11-17 09:20:05.946749: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-11-17 09:20:06.112011: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-11-17 09:20:06.113842: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-17 09:20:07.914032: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary modules\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow_compression as tfc\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# Define the hyperparameters\n",
    "batch_size = 128\n",
    "num_epochs = 10\n",
    "\n",
    "# Load the Fashion MNIST dataset\n",
    "transform = tf.keras.layers.experimental.preprocessing.Rescaling(1./255)\n",
    "trainset = tfds.load('fashion_mnist', split='train', as_supervised=True).map(lambda x, y: (transform(x), y)).batch(batch_size)\n",
    "testset = tfds.load('fashion_mnist', split='test', as_supervised=True).map(lambda x, y: (transform(x), y)).batch(batch_size)\n",
    "classes = ('T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot')\n",
    "\n",
    "# Define the teacher model (a small convolutional neural network)\n",
    "class Teacher(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Teacher, self).__init__()\n",
    "        self.conv1 = tf.keras.layers.Conv2D(16, 3, padding='same', activation='relu')\n",
    "        self.pool1 = tf.keras.layers.MaxPool2D(2, 2)\n",
    "        self.conv2 = tf.keras.layers.Conv2D(32, 3, padding='same', activation='relu')\n",
    "        self.pool2 = tf.keras.layers.MaxPool2D(2, 2)\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.fc1 = tf.keras.layers.Dense(64, activation='relu')\n",
    "        self.fc2 = tf.keras.layers.Dense(10)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.pool1(self.conv1(x))\n",
    "        x = self.pool2(self.conv2(x))\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Define another teacher model (a small deep neural network)\n",
    "class Teacher_DNN(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Teacher_DNN, self).__init__()\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.fc1 = tf.keras.layers.Dense(32, activation='relu')\n",
    "        self.fc2 = tf.keras.layers.Dense(32, activation='relu')\n",
    "        self.fc3 = tf.keras.layers.Dense(10)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Define the student model (a smaller fully-connected neural network)\n",
    "class Student(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Student, self).__init__()\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.fc1 = tf.keras.layers.Dense(32, activation='relu')\n",
    "        self.fc2 = tf.keras.layers.Dense(10)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the models\n",
    "teacher_cnn = Teacher()\n",
    "teacher_dnn = Teacher_DNN()\n",
    "student_simple = Student()\n",
    "student_mimic = Student()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "469/469 [==============================] - 22s 44ms/step - loss: 0.7255 - accuracy: 0.7331 - val_loss: 0.4753 - val_accuracy: 0.8298\n",
      "Epoch 2/10\n",
      "469/469 [==============================] - 13s 29ms/step - loss: 0.4212 - accuracy: 0.8474 - val_loss: 0.4216 - val_accuracy: 0.8465\n",
      "Epoch 3/10\n",
      "469/469 [==============================] - 15s 31ms/step - loss: 0.3632 - accuracy: 0.8698 - val_loss: 0.3661 - val_accuracy: 0.8706\n",
      "Epoch 4/10\n",
      "469/469 [==============================] - 13s 28ms/step - loss: 0.3317 - accuracy: 0.8803 - val_loss: 0.3328 - val_accuracy: 0.8810\n",
      "Epoch 5/10\n",
      "469/469 [==============================] - 15s 32ms/step - loss: 0.3069 - accuracy: 0.8899 - val_loss: 0.3149 - val_accuracy: 0.8874\n",
      "Epoch 6/10\n",
      "469/469 [==============================] - 14s 29ms/step - loss: 0.2892 - accuracy: 0.8955 - val_loss: 0.3043 - val_accuracy: 0.8909\n",
      "Epoch 7/10\n",
      "469/469 [==============================] - 14s 30ms/step - loss: 0.2741 - accuracy: 0.9007 - val_loss: 0.2976 - val_accuracy: 0.8940\n",
      "Epoch 8/10\n",
      "469/469 [==============================] - 14s 29ms/step - loss: 0.2614 - accuracy: 0.9048 - val_loss: 0.2903 - val_accuracy: 0.8984\n",
      "Epoch 9/10\n",
      "469/469 [==============================] - 14s 29ms/step - loss: 0.2496 - accuracy: 0.9090 - val_loss: 0.2879 - val_accuracy: 0.8979\n",
      "Epoch 10/10\n",
      "469/469 [==============================] - 14s 30ms/step - loss: 0.2396 - accuracy: 0.9127 - val_loss: 0.2837 - val_accuracy: 0.9002\n",
      "Finished training the teacher model\n",
      "79/79 [==============================] - 1s 10ms/step - loss: 0.2837 - accuracy: 0.9002\n",
      "Accuracy of the teacher model on the test set: 90.02 %\n"
     ]
    }
   ],
   "source": [
    "# train the teacher CNN model\n",
    "# Define the loss function and the optimizer\n",
    "criterion = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9)\n",
    "\n",
    "# Train the teacher model on the original hard targets\n",
    "teacher_cnn.compile(optimizer=optimizer, loss=criterion, metrics=['accuracy'])\n",
    "teacher_cnn.fit(trainset, epochs=num_epochs, validation_data=testset)\n",
    "print('Finished training the teacher model')\n",
    "\n",
    "# Evaluate the teacher model on the test set\n",
    "teacher_cnn.evaluate(testset)\n",
    "print('Accuracy of the teacher model on the test set: %.2f %%' % (teacher_cnn.metrics[-1].result() * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "469/469 [==============================] - 3s 5ms/step - loss: 0.7046 - accuracy: 0.7592 - val_loss: 0.5188 - val_accuracy: 0.8146\n",
      "Epoch 2/10\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.4746 - accuracy: 0.8336 - val_loss: 0.4747 - val_accuracy: 0.8337\n",
      "Epoch 3/10\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.4339 - accuracy: 0.8482 - val_loss: 0.4573 - val_accuracy: 0.8394\n",
      "Epoch 4/10\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.4088 - accuracy: 0.8561 - val_loss: 0.4411 - val_accuracy: 0.8473\n",
      "Epoch 5/10\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.3890 - accuracy: 0.8630 - val_loss: 0.4240 - val_accuracy: 0.8524\n",
      "Epoch 6/10\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.3732 - accuracy: 0.8680 - val_loss: 0.4122 - val_accuracy: 0.8569\n",
      "Epoch 7/10\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 0.3614 - accuracy: 0.8725 - val_loss: 0.4008 - val_accuracy: 0.8593\n",
      "Epoch 8/10\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.3504 - accuracy: 0.8762 - val_loss: 0.3900 - val_accuracy: 0.8624\n",
      "Epoch 9/10\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.3415 - accuracy: 0.8781 - val_loss: 0.3849 - val_accuracy: 0.8653\n",
      "Epoch 10/10\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.3339 - accuracy: 0.8804 - val_loss: 0.3814 - val_accuracy: 0.8661\n",
      "Finished training the teacher model\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.3814 - accuracy: 0.8661\n",
      "Accuracy of the teacher model on the test set: 86.61 %\n"
     ]
    }
   ],
   "source": [
    "# train the teacher DNN model\n",
    "# Define the loss function and the optimizer\n",
    "criterion = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9)\n",
    "\n",
    "# Train the teacher model on the original hard targets\n",
    "teacher_dnn.compile(optimizer=optimizer, loss=criterion, metrics=['accuracy'])\n",
    "teacher_dnn.fit(trainset, epochs=num_epochs, validation_data=testset)\n",
    "print('Finished training the teacher model')\n",
    "\n",
    "# Evaluate the teacher model on the test set\n",
    "teacher_dnn.evaluate(testset)\n",
    "print('Accuracy of the teacher model on the test set: %.2f %%' % (teacher_dnn.metrics[-1].result() * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   200] loss: 0.859\n",
      "[1,   400] loss: 0.550\n",
      "[2,   200] loss: 0.484\n",
      "[2,   400] loss: 0.469\n",
      "[3,   200] loss: 0.443\n",
      "[3,   400] loss: 0.437\n",
      "[4,   200] loss: 0.420\n",
      "[4,   400] loss: 0.416\n",
      "[5,   200] loss: 0.405\n",
      "[5,   400] loss: 0.402\n",
      "[6,   200] loss: 0.393\n",
      "[6,   400] loss: 0.390\n",
      "[7,   200] loss: 0.382\n",
      "[7,   400] loss: 0.379\n",
      "[8,   200] loss: 0.373\n",
      "[8,   400] loss: 0.369\n",
      "[9,   200] loss: 0.365\n",
      "[9,   400] loss: 0.361\n",
      "[10,   200] loss: 0.357\n",
      "[10,   400] loss: 0.354\n",
      "Finished training the simple student model\n",
      "79/79 [==============================] - 1s 3ms/step - loss: 0.3986 - accuracy: 0.8589\n",
      "Accuracy of the simple student model on the test set: 85.89 %\n"
     ]
    }
   ],
   "source": [
    "temperature = 10 # A scaling factor for the soft targets\n",
    "alpha = 0.9 # A weighting factor for the soft loss\n",
    "\n",
    "student_simple = Student()\n",
    "# Train the student model on the original hard targets\n",
    "# Define the loss function and the optimizer\n",
    "criterion = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9)\n",
    "\n",
    "# Train the student model on the original hard targets from the teacher model\n",
    "student_simple.compile(optimizer=optimizer, loss=criterion, metrics=['accuracy'])\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, (inputs, labels) in enumerate(trainset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            outputs = student_simple(inputs)\n",
    "            with tf.GradientTape(watch_accessed_variables=False) as tape2:\n",
    "                tape2.watch(outputs)\n",
    "                teacher_outputs = teacher_cnn(inputs)\n",
    "            # Compute the soft loss and the hard loss\n",
    "            loss = criterion(labels, outputs)\n",
    "        # Apply the gradients\n",
    "        grads = tape.gradient(loss, student_simple.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, student_simple.trainable_variables))\n",
    "        running_loss += loss.numpy()\n",
    "        if i % 200 == 199:\n",
    "            print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 200))\n",
    "            running_loss = 0.0\n",
    "print('Finished training the simple student model')\n",
    "\n",
    "# Evaluate the simple student model on the test set\n",
    "student_simple.evaluate(testset)\n",
    "print('Accuracy of the simple student model on the test set: %.2f %%' % (student_simple.metrics[-1].result() * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   200] loss: 0.269\n",
      "[1,   400] loss: 0.159\n",
      "[2,   200] loss: 0.121\n",
      "[2,   400] loss: 0.110\n",
      "[3,   200] loss: 0.099\n",
      "[3,   400] loss: 0.094\n",
      "[4,   200] loss: 0.087\n",
      "[4,   400] loss: 0.086\n",
      "[5,   200] loss: 0.081\n",
      "[5,   400] loss: 0.080\n",
      "[6,   200] loss: 0.077\n",
      "[6,   400] loss: 0.076\n",
      "[7,   200] loss: 0.074\n",
      "[7,   400] loss: 0.073\n",
      "[8,   200] loss: 0.071\n",
      "[8,   400] loss: 0.071\n",
      "[9,   200] loss: 0.069\n",
      "[9,   400] loss: 0.069\n",
      "[10,   200] loss: 0.068\n",
      "[10,   400] loss: 0.068\n",
      "Finished training the student model\n",
      "79/79 [==============================] - 1s 4ms/step - loss: 0.5022 - accuracy: 0.8229\n",
      "Accuracy of the student model on the test set: 82.29 %\n"
     ]
    }
   ],
   "source": [
    "# Train the student model on the combined soft and hard targets\n",
    "# Define the loss function and the optimizer\n",
    "criterion = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9)\n",
    "\n",
    "# Get the full list of trainable variables\n",
    "trainable_variables = student_mimic.trainable_variables\n",
    "\n",
    "# Build the optimizer with the full list of trainable variables\n",
    "optimizer.build(trainable_variables)\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9)\n",
    "\n",
    "# Train the student model on the soft targets from the teacher model\n",
    "student_mimic.compile(optimizer=optimizer, loss=criterion, metrics=['accuracy'])\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, (inputs, labels) in enumerate(trainset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            outputs = student_mimic(inputs)\n",
    "            with tf.GradientTape(watch_accessed_variables=False) as tape2:\n",
    "                tape2.watch(outputs)\n",
    "                teacher_outputs = teacher_cnn(inputs)\n",
    "            # Compute the soft loss and the hard loss\n",
    "            soft_loss = tf.keras.losses.KLDivergence()(tf.nn.softmax(teacher_outputs / temperature), tf.nn.softmax(outputs / temperature))\n",
    "            hard_loss = criterion(labels, outputs)\n",
    "            # Combine the soft loss and the hard loss with a weighting factor\n",
    "            loss = alpha * soft_loss + (1 - alpha) * hard_loss\n",
    "        # Apply the gradients\n",
    "        grads = tape.gradient(loss, student_mimic.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, student_mimic.trainable_variables))\n",
    "        running_loss += loss.numpy()\n",
    "        if i % 200 == 199:\n",
    "            print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 200))\n",
    "            running_loss = 0.0\n",
    "print('Finished training the student model')\n",
    "\n",
    "# Evaluate the student model on the test set\n",
    "student_mimic.evaluate(testset)\n",
    "print('Accuracy of the student model on the test set: %.2f %%' % (student_mimic.metrics[-1].result() * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
